{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new `GenericNetworkEnv()` Open AI gym using YAWNING TITAN (YT)\n",
    "\n",
    "This notebook provides an end to end example of creating an environment and training a Proximal Policy Optimisation (PPO) agent within it.\n",
    "\n",
    "For the purposes of this example, we are going to first create an environment that has the same network topology as [Ridley 2017](https://www.nsa.gov/portals/70/documents/resources/everyone/digital-media-center/publications/the-next-wave/TNW-22-1.pdf#page=9)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3 import A2C, DQN, PPO\n",
    "from stable_baselines3.ppo import MlpPolicy as PPOMlp\n",
    "\n",
    "from yawning_titan.envs.generic.core.blue_interface import BlueInterface\n",
    "from yawning_titan.envs.generic.core.red_interface import RedInterface\n",
    "from yawning_titan.envs.generic.generic_env import GenericNetworkEnv\n",
    "from yawning_titan.envs.generic.core.action_loops import ActionLoop\n",
    "from yawning_titan.envs.generic.core.network_interface import NetworkInterface\n",
    "from yawning_titan.networks.network import Network\n",
    "from yawning_titan.config.game_modes import default_game_mode_path\n",
    "from yawning_titan.config.game_config.game_mode import GameMode\n",
    "from yawning_titan.networks import network_creator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the sceanrio's settings file\n",
    "\n",
    "Alongside a network, YT environments also need the scenario settings file. This includes a wide range of configurable parameters that shape how the sceanrio works such as the red agents goal, the blue agents observation space and much more. We'll just use the default one for this tutorial but please read the examples provides in `yawning_titan.config.game_modes` for a feel for the flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_mode = GameMode()\n",
    "game_mode.set_from_yaml(default_game_mode_path(),legacy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Network Representation\n",
    "\n",
    "YAWNING TITAN generic network environments rely on being given a network topology. YT has a number of in-built methods that are capable of generating networks but they can be user supplied. In the example below, we use the `yawning_titan.networks.network_db.default_18_node_network` to load the topology derived from Ridley 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_matrix,positions = network_creator.create_18_node_network()\n",
    "network = Network(matrix=adj_matrix, positions=positions)\n",
    "network.set_from_yaml(default_game_mode_path(),legacy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set entry nodes (optional)\n",
    "\n",
    "You can also optionally supply red entry nodes. These are nodes that the red agent can attack from from timestep 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.entry_nodes.nodes = [\"0\", \"1\", \"2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set high value nodes (optional)\n",
    "\n",
    "You can also optionally supply network high value nodes. These are nodes that, if the red agent takes one, the game ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.high_value_nodes.nodes = [\"5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Network Interface Object\n",
    "\n",
    "The network representation and the sceanrio configuration are then combined together to create a `NetworkInterface()` - This can be thought of as the red and blue agents primary point of interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_interface = NetworkInterface(game_mode=game_mode, network=network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the red and blue agents\n",
    "\n",
    "Now that we have an `NetworkInterface()`, the next stage is to create Red and Blue interfaces to provide agents a means of interacting with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red = RedInterface(network_interface)\n",
    "blue = BlueInterface(network_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment\n",
    "\n",
    "The `NetworkInterface()` can now be combined with the red and blue agent interfaces to create a `GenericNetworkEnv()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GenericNetworkEnv(red, blue, network_interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure that the environment passes checks\n",
    "\n",
    "Once created, it's always worth checking the environment to see if its compliant with OpenAI Gym. For this, we can use the `check_env()` function provided by Stable Baselines 3. Silence means we are all good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env, warn=True)\n",
    "\n",
    "# reset anything changed during the check\n",
    "_ = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(PPOMlp, env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(agent, env, n_eval_episodes=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "848c8f009db9df6e0e1b3a1aa5b08bbcab98ccfd0977252ce3d23bd921ece881"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
